{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "import pandas as pd\n",
    "#testArray = np.array([[1,2,3,4],[2,3,4,1],[3,4,1,2],[4,1,2,3],[3,4,1,2],[4,1,2,3]])\n",
    "testArray = np.array([[1, 2], [2, 1]])\n",
    "np.shape(testArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [2., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simpleNMF(A, k, i):\n",
    "    m, n = np.shape(A)\n",
    "    W = np.random.rand(m, k)\n",
    "    H = np.random.rand(k, n)\n",
    "    for _ in range(i):\n",
    "        W = W * (A @ H.T) / (W @ H @ H.T)\n",
    "        H = H * (W.T @ A) / (W.T @ W @ H)\n",
    "    return W @ H\n",
    "\n",
    "simpleNMF(testArray, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [2., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def thread_function_W(A, W, H, q, i):\n",
    "    q.put((i, W * (A @ H.T) / (W @ H @ H.T)))\n",
    "\n",
    "def thread_function_H(A, W, H, q, i):\n",
    "    q.put((i, H * (W.T @ A) / (W.T @ W @ H)))\n",
    "\n",
    "def naiveParallelNMF(A, k, p, numIter):\n",
    "    m, n = np.shape(A)\n",
    "    if m % p > 0:\n",
    "        raise TypeError('Input first dimension not divisible by number of threads')\n",
    "    if n % p > 0:\n",
    "        raise TypeError('Input second dimension not divisible by number of threads')\n",
    "    W = np.random.rand(m, k)\n",
    "    H = np.random.rand(k, n)\n",
    "    As1 = [A[int(i*m/p):int((i+1)*m/p), :] for i in range(p)]\n",
    "    As2 = [A[:, int(i*n/p):int((i+1)*n/p)] for i in range(p)]\n",
    "    \n",
    "    for _ in range(numIter):\n",
    "        Ws = [W[int(i*m/p):int((i+1)*m/p), :] for i in range(p)] # chop up W\n",
    "        w_threads = []\n",
    "        w_queue = queue.Queue()\n",
    "        for j in range(p):\n",
    "            newThread = threading.Thread(target = thread_function_W, args = (As1[j], Ws[j], H, w_queue, j)) # each thread updates one section\n",
    "            newThread.start()\n",
    "            w_threads.append(newThread)\n",
    "        for thread in w_threads: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not w_queue.empty(): # reconstitute and update W\n",
    "            i, v = w_queue.get()\n",
    "            Ws[i] = v\n",
    "        W = np.concatenate(Ws, 0)\n",
    "\n",
    "        Hs = [H[:, int(i*n/p):int((i+1)*n/p)] for i in range(p)] # same procedure for H\n",
    "        h_threads = []\n",
    "        h_queue = queue.Queue()\n",
    "        for j in range(p):\n",
    "            newThread = threading.Thread(target = thread_function_H, args = (As2[j], W, Hs[j], h_queue, j))\n",
    "            newThread.start()\n",
    "            h_threads.append(newThread)\n",
    "        for thread in h_threads:\n",
    "            thread.join()\n",
    "        while not h_queue.empty():\n",
    "            i, v = h_queue.get()\n",
    "            Hs[i] = v\n",
    "        H = np.concatenate(Hs, 1)\n",
    "    return W @ H\n",
    "    \n",
    "naiveParallelNMF(testArray, 2, 2, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPC_NMF(A, k, p1, p2, numIter):\n",
    "    m, n = np.shape(A)\n",
    "    if m % (p1*p2) > 0:\n",
    "        raise TypeError('Input first dimension not divisible by number of threads')\n",
    "    if n % (p1*p2) > 0:\n",
    "        raise TypeError('Input second dimension not divisible by number of threads')\n",
    "    W = np.random.rand(m, k)\n",
    "    H = np.random.rand(k, n)\n",
    "\n",
    "    Hs = [[np.random.rand(k, int(n/(p1*p2))) for i in range(p1)] for j in range(p2)]\n",
    "\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will optimize both the number of processor dimensions (p) and the rank of the factorization (k). For each, we will optimize runtime, minimize computing costs, bandwith cost(?), and communication cost (?). We will examine these results on both dense and sparse imput matricies (A)(?). For each set, the study fixes the number of processors at 600 and vary the rank of k. We will replicate this, but also try to optimize the number of processors for our dataset.\n",
    "\n",
    "Note** we need to find datasets; preferably one sparse and one dense dataset, or we could generate synthetic sparse and dense datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_01</th>\n",
       "      <th>pos_02</th>\n",
       "      <th>pos_03</th>\n",
       "      <th>pos_04</th>\n",
       "      <th>pos_05</th>\n",
       "      <th>pos_06</th>\n",
       "      <th>pos_07</th>\n",
       "      <th>pos_08</th>\n",
       "      <th>pos_09</th>\n",
       "      <th>pos_10</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_34</th>\n",
       "      <th>pos_35</th>\n",
       "      <th>pos_36</th>\n",
       "      <th>pos_37</th>\n",
       "      <th>pos_38</th>\n",
       "      <th>pos_39</th>\n",
       "      <th>pos_40</th>\n",
       "      <th>pos_41</th>\n",
       "      <th>pos_42</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376640 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos_01  pos_02  pos_03  pos_04  pos_05  pos_06  pos_07  pos_08  \\\n",
       "0          1.0     1.0     1.0     2.0     2.0     1.0     0.0     2.0   \n",
       "1          0.0     0.0     1.0     1.0     1.0     1.0     0.0     0.0   \n",
       "2          0.0     1.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "3          0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "4          0.0     2.0     2.0     2.0     1.0     0.0     0.0     1.0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "376635     0.0     0.0     0.0     2.0     0.0     0.0     0.0     0.0   \n",
       "376636     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "376637     0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0   \n",
       "376638     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "376639     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "        pos_09  pos_10  ...  pos_34  pos_35  pos_36  pos_37  pos_38  pos_39  \\\n",
       "0          2.0     2.0  ...     1.0     1.0     2.0     1.0     2.0     1.0   \n",
       "1          0.0     1.0  ...     2.0     1.0     1.0     2.0     2.0     2.0   \n",
       "2          2.0     0.0  ...     0.0     2.0     1.0     2.0     2.0     1.0   \n",
       "3          0.0     0.0  ...     0.0     1.0     1.0     1.0     2.0     1.0   \n",
       "4          1.0     1.0  ...     0.0     1.0     2.0     2.0     1.0     2.0   \n",
       "...        ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "376635     0.0     2.0  ...     0.0     1.0     2.0     2.0     2.0     1.0   \n",
       "376636     0.0     2.0  ...     0.0     0.0     0.0     2.0     1.0     2.0   \n",
       "376637     0.0     2.0  ...     1.0     1.0     2.0     2.0     2.0     1.0   \n",
       "376638     2.0     0.0  ...     2.0     0.0     2.0     2.0     1.0     1.0   \n",
       "376639     0.0     0.0  ...     2.0     0.0     2.0     2.0     2.0     1.0   \n",
       "\n",
       "        pos_40  pos_41  pos_42  winner  \n",
       "0          2.0     1.0     2.0     2.0  \n",
       "1          1.0     1.0     2.0     1.0  \n",
       "2          2.0     2.0     2.0     2.0  \n",
       "3          2.0     0.0     1.0     2.0  \n",
       "4          1.0     0.0     2.0     1.0  \n",
       "...        ...     ...     ...     ...  \n",
       "376635     2.0     0.0     1.0     2.0  \n",
       "376636     1.0     0.0     0.0     2.0  \n",
       "376637     1.0     1.0     2.0     2.0  \n",
       "376638     1.0     2.0     0.0     2.0  \n",
       "376639     2.0     1.0     2.0     1.0  \n",
       "\n",
       "[376640 rows x 43 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c4 = pd.read_csv('c4_game_database.csv')\n",
    "c4[c4 < 0] = 2 #changing -1 values to 2\n",
    "c4\n",
    "#really not sure if this works for a sparse dataset but wanted to start exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:147: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:147: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/xf/8808rrps3wb3r170fzcq_6t00000gn/T/ipykernel_57360/2333205815.py:147: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  this = self.run(A, k, verbose=(-1 if verbose is 0 else verbose))\n"
     ]
    }
   ],
   "source": [
    "class NMF_Base(object):\n",
    "\n",
    "    \"\"\" Base class for NMF algorithms\n",
    "    Specific algorithms need to be implemented by deriving from this class.\n",
    "    \"\"\"\n",
    "    default_max_iter = 100\n",
    "    default_max_time = np.inf\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\n",
    "            'NMF_Base is a base class that cannot be instantiated')\n",
    "\n",
    "    def set_default(self, default_max_iter, default_max_time):\n",
    "        self.default_max_iter = default_max_iter\n",
    "        self.default_max_time = default_max_time\n",
    "\n",
    "    def run(self, A, k, init=None, max_iter=None, max_time=None, verbose=0):\n",
    "        \"\"\" Run a NMF algorithm\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : numpy.array or scipy.sparse matrix, shape (m,n)\n",
    "        k : int - target lower rank\n",
    "        Optional Parameters\n",
    "        -------------------\n",
    "        init : (W_init, H_init) where\n",
    "                    W_init is numpy.array of shape (m,k) and\n",
    "                    H_init is numpy.array of shape (n,k).\n",
    "                    If provided, these values are used as initial values for NMF iterations.\n",
    "        max_iter : int - maximum number of iterations.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        max_time : int - maximum amount of time in seconds.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        verbose : int - 0 (default) - No debugging information is collected, but\n",
    "                                    input and output information is printed on screen.\n",
    "                        -1 - No debugging information is collected, and\n",
    "                                    nothing is printed on screen.\n",
    "                        1 (debugging/experimental purpose) - History of computation is\n",
    "                                        returned. See 'rec' variable.\n",
    "                        2 (debugging/experimental purpose) - History of computation is\n",
    "                                        additionally printed on screen.\n",
    "        Returns\n",
    "        -------\n",
    "        (W, H, rec)\n",
    "        W : Obtained factor matrix, shape (m,k)\n",
    "        H : Obtained coefficient matrix, shape (n,k)\n",
    "        rec : dict - (debugging/experimental purpose) Auxiliary information about the execution\n",
    "        \"\"\"\n",
    "        info = {'k': k,\n",
    "                'alg': str(self.__class__),\n",
    "                'A_dim_1': A.shape[0],\n",
    "                'A_dim_2': A.shape[1],\n",
    "                'A_type': str(A.__class__),\n",
    "                'max_iter': max_iter if max_iter is not None else self.default_max_iter,\n",
    "                'verbose': verbose,\n",
    "                'max_time': max_time if max_time is not None else self.default_max_time}\n",
    "        if init != None:\n",
    "            W = init[0].copy()\n",
    "            H = init[1].copy()\n",
    "            info['init'] = 'user_provided'\n",
    "        else:\n",
    "            W = random.rand(A.shape[0], k)\n",
    "            H = random.rand(A.shape[1], k)\n",
    "            info['init'] = 'uniform_random'\n",
    "\n",
    "        if verbose >= 0:\n",
    "            print ('[NMF] Running: ')\n",
    "            print (json.dumps(info, indent=4, sort_keys=True))\n",
    "\n",
    "        norm_A = mu.norm_fro(A)\n",
    "        total_time = 0\n",
    "\n",
    "        if verbose >= 1:\n",
    "            his = {'iter': [], 'elapsed': [], 'rel_error': []}\n",
    "\n",
    "        start = time.time()\n",
    "        # algorithm-specific initilization\n",
    "        (W, H) = self.initializer(W, H)\n",
    "\n",
    "        for i in range(1, info['max_iter'] + 1):\n",
    "            start_iter = time.time()\n",
    "            # algorithm-specific iteration solver\n",
    "            (W, H) = self.iter_solver(A, W, H, k, i)\n",
    "            elapsed = time.time() - start_iter\n",
    "\n",
    "            if verbose >= 1:\n",
    "                rel_error = mu.norm_fro_err(A, W, H, norm_A) / norm_A\n",
    "                his['iter'].append(i)\n",
    "                his['elapsed'].append(elapsed)\n",
    "                his['rel_error'].append(rel_error)\n",
    "                if verbose >= 2:\n",
    "                    print ('iter:' + str(i) + ', elapsed:' + str(elapsed) + ', rel_error:' + str(rel_error))\n",
    "\n",
    "            total_time += elapsed\n",
    "            if total_time > info['max_time']:\n",
    "                break\n",
    "\n",
    "        W, H, weights = mu.normalize_column_pair(W, H)\n",
    "\n",
    "        final = {}\n",
    "        final['norm_A'] = norm_A\n",
    "        final['rel_error'] = mu.norm_fro_err(A, W, H, norm_A) / norm_A\n",
    "        final['iterations'] = i\n",
    "        final['elapsed'] = time.time() - start\n",
    "\n",
    "        rec = {'info': info, 'final': final}\n",
    "        if verbose >= 1:\n",
    "            rec['his'] = his\n",
    "\n",
    "        if verbose >= 0:\n",
    "            print ('[NMF] Completed: ')\n",
    "            print (json.dumps(final, indent=4, sort_keys=True))\n",
    "        return (W, H, rec)\n",
    "\n",
    "    def run_repeat(self, A, k, num_trial, max_iter=None, max_time=None, verbose=0):\n",
    "        \"\"\" Run an NMF algorithm several times with random initial values \n",
    "            and return the best result in terms of the Frobenius norm of\n",
    "            the approximation error matrix\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : numpy.array or scipy.sparse matrix, shape (m,n)\n",
    "        k : int - target lower rank\n",
    "        num_trial : int number of trials\n",
    "        Optional Parameters\n",
    "        -------------------\n",
    "        max_iter : int - maximum number of iterations for each trial.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        max_time : int - maximum amount of time in seconds for each trial.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        verbose : int - 0 (default) - No debugging information is collected, but\n",
    "                                    input and output information is printed on screen.\n",
    "                        -1 - No debugging information is collected, and\n",
    "                                    nothing is printed on screen.\n",
    "                        1 (debugging/experimental purpose) - History of computation is\n",
    "                                        returned. See 'rec' variable.\n",
    "                        2 (debugging/experimental purpose) - History of computation is\n",
    "                                        additionally printed on screen.\n",
    "        Returns\n",
    "        -------\n",
    "        (W, H, rec)\n",
    "        W : Obtained factor matrix, shape (m,k)\n",
    "        H : Obtained coefficient matrix, shape (n,k)\n",
    "        rec : dict - (debugging/experimental purpose) Auxiliary information about the execution\n",
    "        \"\"\"\n",
    "        for t in iter(range(num_trial)):\n",
    "            if verbose >= 0:\n",
    "                print ('[NMF] Running the {0}/{1}-th trial ...'.format(t + 1, num_trial))\n",
    "            this = self.run(A, k, verbose=(-1 if verbose is 0 else verbose))\n",
    "            if t == 0:\n",
    "                best = this\n",
    "            else:\n",
    "                if this[2]['final']['rel_error'] < best[2]['final']['rel_error']:\n",
    "                    best = this\n",
    "        if verbose >= 0:\n",
    "            print ('[NMF] Best result is as follows.')\n",
    "            print (json.dumps(best[2]['final'], indent=4, sort_keys=True))\n",
    "        return best\n",
    "\n",
    "    def iter_solver(self, A, W, H, k, it):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initializer(self, W, H):\n",
    "        return (W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "import scipy.optimize as opt\n",
    "import numpy.linalg as nla\n",
    "from nonnegfac import matrix_utils as mu\n",
    "import time\n",
    "import json\n",
    "from numpy import random\n",
    "from nonnegfac.nnls import nnlsm_activeset\n",
    "from nonnegfac.nnls import nnlsm_blockpivot\n",
    "class NMF_HALS(NMF_Base):\n",
    "\n",
    "    \"\"\" NMF algorithm: Hierarchical alternating least squares\n",
    "    A. Cichocki and A.-H. Phan, Fast local algorithms for large scale nonnegative matrix and tensor factorizations,\n",
    "    IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences,\n",
    "    vol. E92-A, no. 3, pp. 708-721, 2009.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, default_max_iter=100, default_max_time=np.inf):\n",
    "        self.eps = 1e-16\n",
    "        self.set_default(default_max_iter, default_max_time)\n",
    "\n",
    "    def initializer(self, W, H):\n",
    "        W, H, weights = mu.normalize_column_pair(W, H)\n",
    "        return W, H\n",
    "\n",
    "    def iter_solver(self, A, W, H, k, it):\n",
    "        AtW = A.T.dot(W)\n",
    "        WtW = W.T.dot(W)\n",
    "        for kk in iter(range(0, k)):\n",
    "            temp_vec = H[:, kk] + AtW[:, kk] - H.dot(WtW[:, kk])\n",
    "            H[:, kk] = np.maximum(temp_vec, self.eps)\n",
    "\n",
    "        AH = A.dot(H)\n",
    "        HtH = H.T.dot(H)\n",
    "        for kk in iter(range(0, k)):\n",
    "            temp_vec = W[:, kk] * HtH[kk, kk] + AH[:, kk] - W.dot(HtH[:, kk])\n",
    "            W[:, kk] = np.maximum(temp_vec, self.eps)\n",
    "            ss = nla.norm(W[:, kk])\n",
    "            if ss > 0:\n",
    "                W[:, kk] = W[:, kk] / ss\n",
    "\n",
    "        return (W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce49416ec5583b0c8275b442a1d04bbfd540648d21a9ae0fe7642419cc3fbdff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
