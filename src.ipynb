{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 2., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 2., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 2., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 2., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 2., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 2., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 2.]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_NMF(a, k, i):\n",
    "    m, n = np.shape(a)\n",
    "    w = np.random.rand(m, k)\n",
    "    h = np.random.rand(k, n)\n",
    "    for _ in range(i):\n",
    "        w = w * (a @ h.T) / (w @ h @ h.T)\n",
    "        h = h * (w.T @ a) / (w.T @ w @ h)\n",
    "    return w @ h\n",
    "\n",
    "simple_NMF(np.identity(8)+1, 8, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function_w(a, w, h, q, i):\n",
    "    q.put((i, w * (a @ h.T) / (w @ h @ h.T)))\n",
    "\n",
    "def thread_function_h(a, w, h, q, i):\n",
    "    q.put((i, h * (w.T @ a) / (w.T @ w @ h)))\n",
    "\n",
    "def naive_parallel_NMF(a, k, p, numIter):\n",
    "    m, n = np.shape(a)\n",
    "    if m % p > 0:\n",
    "        raise TypeError('Input first dimension not divisible by number of threads')\n",
    "    if n % p > 0:\n",
    "        raise TypeError('Input second dimension not divisible by number of threads')\n",
    "    w = np.random.rand(m, k)\n",
    "    h = np.random.rand(k, n)\n",
    "    a_pieces_1 = np.split(a, p, 0) # cut a into p pieces of shape m/p x n\n",
    "    assert np.shape(a_pieces_1[0]) == (int(m/p), n)\n",
    "    a_pieces_2 = np.split(a, p, 1) # cut a into p pieces of shape m x n/p\n",
    "    assert np.shape(a_pieces_2[0]) == (m, int(n/p))\n",
    "    \n",
    "    for _ in range(numIter):\n",
    "        w_pieces = np.split(w, p, 0) # cut w into p pieces of shape m/p x n\n",
    "        assert np.shape(w_pieces[0]) == (int(m/p), k)\n",
    "        w_threads = []\n",
    "        w_queue = queue.Queue()\n",
    "        for j in range(p): # split into p threads to calculate updates for each piece\n",
    "            newThread = threading.Thread(target = thread_function_w, args = (a_pieces_1[j], w_pieces[j], h, w_queue, j))\n",
    "            newThread.start()\n",
    "            w_threads.append(newThread)\n",
    "        for thread in w_threads: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not w_queue.empty(): # reconstitute and update w\n",
    "            i, val = w_queue.get()\n",
    "            w_pieces[i] = val\n",
    "        w = np.concatenate(w_pieces, 0)\n",
    "\n",
    "        h_pieces = np.split(h, p, 1) # cut h into p pieces of shape m x n/p\n",
    "        assert np.shape(h_pieces[0]) == (k, int(n/p))\n",
    "        h_threads = []\n",
    "        h_queue = queue.Queue()\n",
    "        for j in range(p): # split into p threads to calculate updates for each piece\n",
    "            newThread = threading.Thread(target = thread_function_h, args = (a_pieces_2[j], w, h_pieces[j], h_queue, j))\n",
    "            newThread.start()\n",
    "            h_threads.append(newThread)\n",
    "        for thread in h_threads: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not h_queue.empty(): # reconstitute and update h\n",
    "            i, val = h_queue.get()\n",
    "            h_pieces[i] = val\n",
    "        h = np.concatenate(h_pieces, 1)\n",
    "    return w @ h\n",
    "    \n",
    "#naive_parallel_NMF(np.identity(8)+1, 8, 4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 2., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 2., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 2., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 2., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 2., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 2., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 2.]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vertical_thread_function_u(h, q):\n",
    "    q.put(h @ h.T)\n",
    "\n",
    "def vertical_thread_function_v(a, h, q, i):\n",
    "    q.put((i, a @ h.T))\n",
    "\n",
    "def vertical_thread_function_w(w, u, y, q, i):\n",
    "    q.put((i, w * y / (w @ u)))\n",
    "\n",
    "def vertical_thread_function_x(w, q):\n",
    "    q.put(w.T @ w)\n",
    "\n",
    "def vertical_thread_function_y(a, w, q, i):\n",
    "    q.put((i, w.T @ a))\n",
    "\n",
    "def vertical_thread_function_h(h, x, y, q, i):\n",
    "    q.put((i, h * y / (x @ h)))\n",
    "\n",
    "def vertical_HPC_NMF(a, k, p_row, p_col, numIter):\n",
    "    m, n = np.shape(a)\n",
    "    if m % (p_row*p_col) > 0:\n",
    "        raise TypeError('Input first dimension not divisible by number of threads')\n",
    "    if n % (p_row*p_col) > 0:\n",
    "        raise TypeError('Input second dimension not divisible by number of threads')\n",
    "    w = np.random.rand(m, k)\n",
    "    h = np.random.rand(k, n)\n",
    "\n",
    "    a_pieces = [np.split(x, p_col, 1) for x in np.split(a, p_row, 0)] # cut a into p_row x p_col pieces of shape m/p_row x n/p_col\n",
    "    assert np.shape(a_pieces[0][0]) == (int(m/p_row), int(n/p_col))\n",
    "\n",
    "    for _ in range(numIter):\n",
    "        u = np.zeros((k, k))\n",
    "        h_pieces_u = np.split(h, p_row*p_col, 1) # cut h into p_row*p_col pieces of shape k x n/(p_row*p_col)\n",
    "        assert np.shape(h_pieces_u[0]) == (k, int(n/(p_row*p_col)))\n",
    "        threads_u = []\n",
    "        queue_u = queue.Queue()\n",
    "        for i in range(p_row*p_col): # split into p_row*p_col threads to calculate u for each piece\n",
    "            newThread = threading.Thread(target = vertical_thread_function_u, args = (h_pieces_u[i], queue_u))\n",
    "            newThread.start()\n",
    "            threads_u.append(newThread)\n",
    "        for thread in threads_u: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not queue_u.empty(): # sum up u\n",
    "            u += queue_u.get()\n",
    "        assert np.all(np.isclose(u, h @ h.T))       \n",
    "\n",
    "        v = np.zeros((m, k))\n",
    "        v_pieces = np.split(v, p_row, 0) # cut v into p_row pieces of shape m/p_row x k\n",
    "        assert np.shape(v_pieces[0]) == (int(m/p_row), k)\n",
    "        h_pieces_v = np.split(h, p_col, 1) # cut h into p_col pieces of shape k x n/p_col\n",
    "        assert np.shape(h_pieces_v[0]) == (k, int(n/p_col))\n",
    "        threads_v = []\n",
    "        queue_v = queue.Queue()\n",
    "        for i in range(p_row*p_col): # split into p_row*p_col threads to calculate updates for each piece\n",
    "            newThread = threading.Thread(target = vertical_thread_function_v, args = (a_pieces[int(i/p_col)][i%p_col], h_pieces_v[i%p_col], queue_v, i))\n",
    "            newThread.start()\n",
    "            threads_v.append(newThread)\n",
    "        for thread in threads_v: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not queue_v.empty(): # sum up v\n",
    "            i, val = queue_v.get()\n",
    "            v_pieces[int(i/p_col)] += val\n",
    "        v = np.concatenate(v_pieces, 0)\n",
    "        assert np.all(np.isclose(v, a @ h.T))\n",
    "        \n",
    "        w_pieces = np.split(w, p_row*p_col, 0) # cut w into p_row x p_col pieces of shape m/(p_row*p_col) x k\n",
    "        assert np.shape(w_pieces[0]) == (m/(p_row*p_col), k)\n",
    "        v_pieces_w = np.split(v, p_row*p_col, 0) # cut v into p_row x p_col pieces of shape m/(p_row*p_col) x k\n",
    "        assert np.shape(v_pieces_w[0]) == (m/(p_row*p_col), k)\n",
    "        threads_w = []\n",
    "        queue_w = queue.Queue()\n",
    "        for i in range(p_row*p_col): # split into p_row*p_col threads to calculate updates for each piece\n",
    "            newThread = threading.Thread(target = vertical_thread_function_w, args = (w_pieces[i], u, v_pieces_w[i], queue_w, i))\n",
    "            newThread.start()\n",
    "            threads_w.append(newThread)\n",
    "        for thread in threads_w: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not queue_w.empty(): # sum up u\n",
    "            i, val = queue_w.get()\n",
    "            w_pieces[i] = val\n",
    "        w = np.concatenate(w_pieces, 0)\n",
    "        \n",
    "        x = np.zeros((k, k))\n",
    "        w_pieces_x = np.split(w, p_row*p_col, 0) # cut x into p_row*p_col pieces of shape n/(p_row*p_col) x k\n",
    "        assert np.shape(w_pieces_x[0]) == (int(n/(p_row*p_col)), k)\n",
    "        threads_x = []\n",
    "        queue_x = queue.Queue()\n",
    "        for i in range(p_row*p_col): # split into p_row*p_col threads to calculate u for each piece\n",
    "            newThread = threading.Thread(target = vertical_thread_function_x, args = (w_pieces_x[i], queue_x))\n",
    "            newThread.start()\n",
    "            threads_x.append(newThread)\n",
    "        for thread in threads_x: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not queue_x.empty(): # sum up x\n",
    "            x += queue_x.get()\n",
    "        assert np.all(np.isclose(x, w.T @ w))\n",
    "\n",
    "        y = np.zeros((k, n))\n",
    "        y_pieces = np.split(y, p_col, 1) # cut y into p_col pieces of shape k x n/p_col\n",
    "        assert np.shape(y_pieces[0]) == (k, int(n/p_col))\n",
    "        w_pieces_y = np.split(w, p_row, 0) # cut w into p_row pieces of shape m/p_row x k\n",
    "        assert np.shape(w_pieces_y[0]) == (int(m/p_row), k)\n",
    "        threads_y = []\n",
    "        queue_y = queue.Queue()\n",
    "        for i in range(p_row*p_col): # split into p_row*p_col threads to calculate updates for each piece\n",
    "            newThread = threading.Thread(target = vertical_thread_function_y, args = (a_pieces[i%p_row][int(i/p_row)], w_pieces_y[i%p_row], queue_y, i))\n",
    "            newThread.start()\n",
    "            threads_y.append(newThread)\n",
    "        for thread in threads_y: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not queue_y.empty(): # sum up y\n",
    "            i, val = queue_y.get()\n",
    "            y_pieces[int(i/p_row)] += val\n",
    "        y = np.concatenate(y_pieces, 1)\n",
    "        assert np.all(np.isclose(y, w.T @ a))\n",
    "\n",
    "        h_pieces = np.split(h, p_row*p_col, 1) # cut h into p_row x p_col pieces of shape m/(p_row*p_col) x k\n",
    "        assert np.shape(h_pieces[0]) == (k, int(n/(p_row*p_col)))\n",
    "        y_pieces_h = np.split(y, p_row*p_col, 1) # cut y into p_row x p_col pieces of shape m/(p_row*p_col) x k\n",
    "        assert np.shape(y_pieces_h[0]) == (k, int(n/(p_row*p_col)))\n",
    "        threads_h = []\n",
    "        queue_h = queue.Queue()\n",
    "        for i in range(p_row*p_col): # split into p_row*p_col threads to calculate updates for each piece\n",
    "            newThread = threading.Thread(target = vertical_thread_function_h, args = (h_pieces[i], x, y_pieces_h[i], queue_h, i))\n",
    "            newThread.start()\n",
    "            threads_h.append(newThread)\n",
    "        for thread in threads_h: # wait for all threads to complete\n",
    "            thread.join()\n",
    "        while not queue_h.empty(): # sum up u\n",
    "            i, val = queue_h.get()\n",
    "            h_pieces[i] = val\n",
    "        h = np.concatenate(h_pieces, 1)        \n",
    "    return w @ h\n",
    "\n",
    "vertical_HPC_NMF(np.identity(8)+1, 8, 4, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timingFunc(func, args, iterations):\n",
    "    times = []\n",
    "    for _ in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        func(*args)\n",
    "        times.append(time.perf_counter()-start)\n",
    "    return np.mean(times)\n",
    "\n",
    "k = 4\n",
    "p0 = 2\n",
    "p1 = 2\n",
    "iter = 100\n",
    "points = 10\n",
    "sizes = range(5)\n",
    "simple = [timingFunc(simple_NMF, (np.identity(4*i)+1, k, iter), points) for i in sizes]\n",
    "naive = [timingFunc(naive_parallel_NMF, (np.identity(4*i)+1, k, p0*p1, iter), points) for i in sizes]\n",
    "vertical = [timingFunc(vertical_HPC_NMF, (np.identity(4*i)+1, k, p0, p1, iter), points) for i in sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(simple, 'ko')\n",
    "plt.plot(naive, 'ro')\n",
    "plt.plot(vertical, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will optimize both the number of processor dimensions (p) and the rank of the factorization (k). For each, we will optimize runtime, minimize computing costs, bandwith cost(?), and communication cost (?). We will examine these results on both dense and sparse imput matricies (A)(?). For each set, the study fixes the number of processors at 600 and vary the rank of k. We will replicate this, but also try to optimize the number of processors for our dataset.\n",
    "\n",
    "Note** we need to find datasets; preferably one sparse and one dense dataset, or we could generate synthetic sparse and dense datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_01</th>\n",
       "      <th>pos_02</th>\n",
       "      <th>pos_03</th>\n",
       "      <th>pos_04</th>\n",
       "      <th>pos_05</th>\n",
       "      <th>pos_06</th>\n",
       "      <th>pos_07</th>\n",
       "      <th>pos_08</th>\n",
       "      <th>pos_09</th>\n",
       "      <th>pos_10</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_34</th>\n",
       "      <th>pos_35</th>\n",
       "      <th>pos_36</th>\n",
       "      <th>pos_37</th>\n",
       "      <th>pos_38</th>\n",
       "      <th>pos_39</th>\n",
       "      <th>pos_40</th>\n",
       "      <th>pos_41</th>\n",
       "      <th>pos_42</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376640 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos_01  pos_02  pos_03  pos_04  pos_05  pos_06  pos_07  pos_08  \\\n",
       "0          1.0     1.0     1.0     2.0     2.0     1.0     0.0     2.0   \n",
       "1          0.0     0.0     1.0     1.0     1.0     1.0     0.0     0.0   \n",
       "2          0.0     1.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "3          0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "4          0.0     2.0     2.0     2.0     1.0     0.0     0.0     1.0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "376635     0.0     0.0     0.0     2.0     0.0     0.0     0.0     0.0   \n",
       "376636     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "376637     0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0   \n",
       "376638     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "376639     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "        pos_09  pos_10  ...  pos_34  pos_35  pos_36  pos_37  pos_38  pos_39  \\\n",
       "0          2.0     2.0  ...     1.0     1.0     2.0     1.0     2.0     1.0   \n",
       "1          0.0     1.0  ...     2.0     1.0     1.0     2.0     2.0     2.0   \n",
       "2          2.0     0.0  ...     0.0     2.0     1.0     2.0     2.0     1.0   \n",
       "3          0.0     0.0  ...     0.0     1.0     1.0     1.0     2.0     1.0   \n",
       "4          1.0     1.0  ...     0.0     1.0     2.0     2.0     1.0     2.0   \n",
       "...        ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "376635     0.0     2.0  ...     0.0     1.0     2.0     2.0     2.0     1.0   \n",
       "376636     0.0     2.0  ...     0.0     0.0     0.0     2.0     1.0     2.0   \n",
       "376637     0.0     2.0  ...     1.0     1.0     2.0     2.0     2.0     1.0   \n",
       "376638     2.0     0.0  ...     2.0     0.0     2.0     2.0     1.0     1.0   \n",
       "376639     0.0     0.0  ...     2.0     0.0     2.0     2.0     2.0     1.0   \n",
       "\n",
       "        pos_40  pos_41  pos_42  winner  \n",
       "0          2.0     1.0     2.0     2.0  \n",
       "1          1.0     1.0     2.0     1.0  \n",
       "2          2.0     2.0     2.0     2.0  \n",
       "3          2.0     0.0     1.0     2.0  \n",
       "4          1.0     0.0     2.0     1.0  \n",
       "...        ...     ...     ...     ...  \n",
       "376635     2.0     0.0     1.0     2.0  \n",
       "376636     1.0     0.0     0.0     2.0  \n",
       "376637     1.0     1.0     2.0     2.0  \n",
       "376638     1.0     2.0     0.0     2.0  \n",
       "376639     2.0     1.0     2.0     1.0  \n",
       "\n",
       "[376640 rows x 43 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c4 = pd.read_csv('c4_game_database.csv')\n",
    "c4[c4 < 0] = 2 #changing -1 values to 2\n",
    "c4\n",
    "#really not sure if this works for a sparse dataset but wanted to start exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:147: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:147: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/xf/8808rrps3wb3r170fzcq_6t00000gn/T/ipykernel_57360/2333205815.py:147: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  this = self.run(A, k, verbose=(-1 if verbose is 0 else verbose))\n"
     ]
    }
   ],
   "source": [
    "class NMF_Base(object):\n",
    "\n",
    "    \"\"\" Base class for NMF algorithms\n",
    "    Specific algorithms need to be implemented by deriving from this class.\n",
    "    \"\"\"\n",
    "    default_max_iter = 100\n",
    "    default_max_time = np.inf\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\n",
    "            'NMF_Base is a base class that cannot be instantiated')\n",
    "\n",
    "    def set_default(self, default_max_iter, default_max_time):\n",
    "        self.default_max_iter = default_max_iter\n",
    "        self.default_max_time = default_max_time\n",
    "\n",
    "    def run(self, A, k, init=None, max_iter=None, max_time=None, verbose=0):\n",
    "        \"\"\" Run a NMF algorithm\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : numpy.array or scipy.sparse matrix, shape (m,n)\n",
    "        k : int - target lower rank\n",
    "        Optional Parameters\n",
    "        -------------------\n",
    "        init : (W_init, H_init) where\n",
    "                    W_init is numpy.array of shape (m,k) and\n",
    "                    H_init is numpy.array of shape (n,k).\n",
    "                    If provided, these values are used as initial values for NMF iterations.\n",
    "        max_iter : int - maximum number of iterations.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        max_time : int - maximum amount of time in seconds.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        verbose : int - 0 (default) - No debugging information is collected, but\n",
    "                                    input and output information is printed on screen.\n",
    "                        -1 - No debugging information is collected, and\n",
    "                                    nothing is printed on screen.\n",
    "                        1 (debugging/experimental purpose) - History of computation is\n",
    "                                        returned. See 'rec' variable.\n",
    "                        2 (debugging/experimental purpose) - History of computation is\n",
    "                                        additionally printed on screen.\n",
    "        Returns\n",
    "        -------\n",
    "        (W, H, rec)\n",
    "        W : Obtained factor matrix, shape (m,k)\n",
    "        H : Obtained coefficient matrix, shape (n,k)\n",
    "        rec : dict - (debugging/experimental purpose) Auxiliary information about the execution\n",
    "        \"\"\"\n",
    "        info = {'k': k,\n",
    "                'alg': str(self.__class__),\n",
    "                'A_dim_1': A.shape[0],\n",
    "                'A_dim_2': A.shape[1],\n",
    "                'A_type': str(A.__class__),\n",
    "                'max_iter': max_iter if max_iter is not None else self.default_max_iter,\n",
    "                'verbose': verbose,\n",
    "                'max_time': max_time if max_time is not None else self.default_max_time}\n",
    "        if init != None:\n",
    "            W = init[0].copy()\n",
    "            H = init[1].copy()\n",
    "            info['init'] = 'user_provided'\n",
    "        else:\n",
    "            W = random.rand(A.shape[0], k)\n",
    "            H = random.rand(A.shape[1], k)\n",
    "            info['init'] = 'uniform_random'\n",
    "\n",
    "        if verbose >= 0:\n",
    "            print ('[NMF] Running: ')\n",
    "            print (json.dumps(info, indent=4, sort_keys=True))\n",
    "\n",
    "        norm_A = mu.norm_fro(A)\n",
    "        total_time = 0\n",
    "\n",
    "        if verbose >= 1:\n",
    "            his = {'iter': [], 'elapsed': [], 'rel_error': []}\n",
    "\n",
    "        start = time.time()\n",
    "        # algorithm-specific initilization\n",
    "        (W, H) = self.initializer(W, H)\n",
    "\n",
    "        for i in range(1, info['max_iter'] + 1):\n",
    "            start_iter = time.time()\n",
    "            # algorithm-specific iteration solver\n",
    "            (W, H) = self.iter_solver(A, W, H, k, i)\n",
    "            elapsed = time.time() - start_iter\n",
    "\n",
    "            if verbose >= 1:\n",
    "                rel_error = mu.norm_fro_err(A, W, H, norm_A) / norm_A\n",
    "                his['iter'].append(i)\n",
    "                his['elapsed'].append(elapsed)\n",
    "                his['rel_error'].append(rel_error)\n",
    "                if verbose >= 2:\n",
    "                    print ('iter:' + str(i) + ', elapsed:' + str(elapsed) + ', rel_error:' + str(rel_error))\n",
    "\n",
    "            total_time += elapsed\n",
    "            if total_time > info['max_time']:\n",
    "                break\n",
    "\n",
    "        W, H, weights = mu.normalize_column_pair(W, H)\n",
    "\n",
    "        final = {}\n",
    "        final['norm_A'] = norm_A\n",
    "        final['rel_error'] = mu.norm_fro_err(A, W, H, norm_A) / norm_A\n",
    "        final['iterations'] = i\n",
    "        final['elapsed'] = time.time() - start\n",
    "\n",
    "        rec = {'info': info, 'final': final}\n",
    "        if verbose >= 1:\n",
    "            rec['his'] = his\n",
    "\n",
    "        if verbose >= 0:\n",
    "            print ('[NMF] Completed: ')\n",
    "            print (json.dumps(final, indent=4, sort_keys=True))\n",
    "        return (W, H, rec)\n",
    "\n",
    "    def run_repeat(self, A, k, num_trial, max_iter=None, max_time=None, verbose=0):\n",
    "        \"\"\" Run an NMF algorithm several times with random initial values \n",
    "            and return the best result in terms of the Frobenius norm of\n",
    "            the approximation error matrix\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : numpy.array or scipy.sparse matrix, shape (m,n)\n",
    "        k : int - target lower rank\n",
    "        num_trial : int number of trials\n",
    "        Optional Parameters\n",
    "        -------------------\n",
    "        max_iter : int - maximum number of iterations for each trial.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        max_time : int - maximum amount of time in seconds for each trial.\n",
    "                    If not provided, default maximum for each algorithm is used.\n",
    "        verbose : int - 0 (default) - No debugging information is collected, but\n",
    "                                    input and output information is printed on screen.\n",
    "                        -1 - No debugging information is collected, and\n",
    "                                    nothing is printed on screen.\n",
    "                        1 (debugging/experimental purpose) - History of computation is\n",
    "                                        returned. See 'rec' variable.\n",
    "                        2 (debugging/experimental purpose) - History of computation is\n",
    "                                        additionally printed on screen.\n",
    "        Returns\n",
    "        -------\n",
    "        (W, H, rec)\n",
    "        W : Obtained factor matrix, shape (m,k)\n",
    "        H : Obtained coefficient matrix, shape (n,k)\n",
    "        rec : dict - (debugging/experimental purpose) Auxiliary information about the execution\n",
    "        \"\"\"\n",
    "        for t in iter(range(num_trial)):\n",
    "            if verbose >= 0:\n",
    "                print ('[NMF] Running the {0}/{1}-th trial ...'.format(t + 1, num_trial))\n",
    "            this = self.run(A, k, verbose=(-1 if verbose is 0 else verbose))\n",
    "            if t == 0:\n",
    "                best = this\n",
    "            else:\n",
    "                if this[2]['final']['rel_error'] < best[2]['final']['rel_error']:\n",
    "                    best = this\n",
    "        if verbose >= 0:\n",
    "            print ('[NMF] Best result is as follows.')\n",
    "            print (json.dumps(best[2]['final'], indent=4, sort_keys=True))\n",
    "        return best\n",
    "\n",
    "    def iter_solver(self, A, W, H, k, it):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initializer(self, W, H):\n",
    "        return (W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "import scipy.optimize as opt\n",
    "import numpy.linalg as nla\n",
    "from nonnegfac import matrix_utils as mu\n",
    "import time\n",
    "import json\n",
    "from numpy import random\n",
    "from nonnegfac.nnls import nnlsm_activeset\n",
    "from nonnegfac.nnls import nnlsm_blockpivot\n",
    "class NMF_HALS(NMF_Base):\n",
    "\n",
    "    \"\"\" NMF algorithm: Hierarchical alternating least squares\n",
    "    A. Cichocki and A.-H. Phan, Fast local algorithms for large scale nonnegative matrix and tensor factorizations,\n",
    "    IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences,\n",
    "    vol. E92-A, no. 3, pp. 708-721, 2009.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, default_max_iter=100, default_max_time=np.inf):\n",
    "        self.eps = 1e-16\n",
    "        self.set_default(default_max_iter, default_max_time)\n",
    "\n",
    "    def initializer(self, W, H):\n",
    "        W, H, weights = mu.normalize_column_pair(W, H)\n",
    "        return W, H\n",
    "\n",
    "    def iter_solver(self, A, W, H, k, it):\n",
    "        AtW = A.T.dot(W)\n",
    "        WtW = W.T.dot(W)\n",
    "        for kk in iter(range(0, k)):\n",
    "            temp_vec = H[:, kk] + AtW[:, kk] - H.dot(WtW[:, kk])\n",
    "            H[:, kk] = np.maximum(temp_vec, self.eps)\n",
    "\n",
    "        AH = A.dot(H)\n",
    "        HtH = H.T.dot(H)\n",
    "        for kk in iter(range(0, k)):\n",
    "            temp_vec = W[:, kk] * HtH[kk, kk] + AH[:, kk] - W.dot(HtH[:, kk])\n",
    "            W[:, kk] = np.maximum(temp_vec, self.eps)\n",
    "            ss = nla.norm(W[:, kk])\n",
    "            if ss > 0:\n",
    "                W[:, kk] = W[:, kk] / ss\n",
    "\n",
    "        return (W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce49416ec5583b0c8275b442a1d04bbfd540648d21a9ae0fe7642419cc3fbdff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
